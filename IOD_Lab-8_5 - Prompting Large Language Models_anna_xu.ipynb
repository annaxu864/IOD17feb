{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b",
   "metadata": {
    "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612",
   "metadata": {
    "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612"
   },
   "source": [
    "# Lab 8.5 - Prompting Large Language Models\n",
    "\n",
    "In this lab we will practise prompting with a few Large Language Models (LLMs) using Groq (not to be confused with Grok). Groq is a platform that provides access to their custom-built AI hardware via APIs, allowing users to run open-source models such as Llama.\n",
    "\n",
    "We shall see that while LLMs are powerful tools, how you ask a question or frame a task can dramatically influence the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a",
   "metadata": {
    "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719",
   "metadata": {
    "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719"
   },
   "source": [
    "Step 1: Sign up for a free Groq account at https://console.groq.com/home .\n",
    "\n",
    "Step 2: Create a new API key at https://console.groq.com/keys. Copy-paste it into an empty text file called 'groq_key.txt'.\n",
    "\n",
    "Running the next cell will then read in this key and assign it to the variable `groq_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56441b82-ddc5-46e9-9583-878e8a807b7f",
   "metadata": {
    "id": "56441b82-ddc5-46e9-9583-878e8a807b7f"
   },
   "outputs": [],
   "source": [
    "groqfilename = r'/Users/annaxu/Documents/Data Science/Module 8/Labs/groq_key.txt' # this file contains a single line containing your Groq API key only\n",
    "try:\n",
    "    with open(groqfilename, 'r') as f:\n",
    "        groq_key = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    print(\"'%s' file not found\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88",
   "metadata": {
    "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /opt/anaconda3/lib/python3.12/site-packages (0.29.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/anaconda3/lib/python3.12/site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c",
   "metadata": {
    "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227",
   "metadata": {
    "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227"
   },
   "source": [
    "First create an instance of the Groq client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b",
   "metadata": {
    "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b"
   },
   "outputs": [],
   "source": [
    "client = Groq(api_key=groq_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097cd77-7a8d-4502-ae6e-607782706b26",
   "metadata": {
    "id": "a097cd77-7a8d-4502-ae6e-607782706b26"
   },
   "source": [
    "The following code shows what models are currently accessible through Groq. `context_window` refers to the size of memory (in tokens) during a session and `max_completion_tokens` is the maximum number of tokens that are generated in an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4",
   "metadata": {
    "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>active</th>\n",
       "      <th>context_window</th>\n",
       "      <th>public_apps</th>\n",
       "      <th>max_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-86m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632165</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-22m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632101</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>qwen/qwen3-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1748396646</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>40960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meta-llama/llama-guard-4-12b</td>\n",
       "      <td>model</td>\n",
       "      <td>1746743847</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743877158</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>meta-llama/llama-4-scout-17b-16e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743874824</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>compound-beta-mini</td>\n",
       "      <td>model</td>\n",
       "      <td>1742953279</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>qwen-qwq-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1741214760</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>compound-beta</td>\n",
       "      <td>model</td>\n",
       "      <td>1740880017</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>playai-tts-arabic</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682783</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>playai-tts</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682771</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mistral-saba-24b</td>\n",
       "      <td>model</td>\n",
       "      <td>1739996492</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>True</td>\n",
       "      <td>32768</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737924940</td>\n",
       "      <td>DeepSeek / Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>allam-2-7b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737672203</td>\n",
       "      <td>SDAIA</td>\n",
       "      <td>True</td>\n",
       "      <td>4096</td>\n",
       "      <td>None</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>model</td>\n",
       "      <td>1733447754</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>whisper-large-v3-turbo</td>\n",
       "      <td>model</td>\n",
       "      <td>1728413088</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whisper-large-v3</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>llama3-70b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distil-whisper-large-v3-en</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Hugging Face</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3-8b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id object     created  \\\n",
       "18            meta-llama/llama-prompt-guard-2-86m  model  1748632165   \n",
       "1             meta-llama/llama-prompt-guard-2-22m  model  1748632101   \n",
       "9                                  qwen/qwen3-32b  model  1748396646   \n",
       "10                   meta-llama/llama-guard-4-12b  model  1746743847   \n",
       "7   meta-llama/llama-4-maverick-17b-128e-instruct  model  1743877158   \n",
       "17      meta-llama/llama-4-scout-17b-16e-instruct  model  1743874824   \n",
       "16                             compound-beta-mini  model  1742953279   \n",
       "20                                   qwen-qwq-32b  model  1741214760   \n",
       "11                                  compound-beta  model  1740880017   \n",
       "5                               playai-tts-arabic  model  1740682783   \n",
       "21                                     playai-tts  model  1740682771   \n",
       "13                               mistral-saba-24b  model  1739996492   \n",
       "12                  deepseek-r1-distill-llama-70b  model  1737924940   \n",
       "6                                      allam-2-7b  model  1737672203   \n",
       "4                         llama-3.3-70b-versatile  model  1733447754   \n",
       "14                         whisper-large-v3-turbo  model  1728413088   \n",
       "8                                    gemma2-9b-it  model  1693721698   \n",
       "15                           llama-3.1-8b-instant  model  1693721698   \n",
       "3                                whisper-large-v3  model  1693721698   \n",
       "19                                llama3-70b-8192  model  1693721698   \n",
       "2                      distil-whisper-large-v3-en  model  1693721698   \n",
       "0                                  llama3-8b-8192  model  1693721698   \n",
       "\n",
       "           owned_by  active  context_window public_apps  max_completion_tokens  \n",
       "18             Meta    True             512        None                    512  \n",
       "1              Meta    True             512        None                    512  \n",
       "9     Alibaba Cloud    True          131072        None                  40960  \n",
       "10             Meta    True          131072        None                   1024  \n",
       "7              Meta    True          131072        None                   8192  \n",
       "17             Meta    True          131072        None                   8192  \n",
       "16             Groq    True          131072        None                   8192  \n",
       "20    Alibaba Cloud    True          131072        None                 131072  \n",
       "11             Groq    True          131072        None                   8192  \n",
       "5            PlayAI    True            8192        None                   8192  \n",
       "21           PlayAI    True            8192        None                   8192  \n",
       "13       Mistral AI    True           32768        None                  32768  \n",
       "12  DeepSeek / Meta    True          131072        None                 131072  \n",
       "6             SDAIA    True            4096        None                   4096  \n",
       "4              Meta    True          131072        None                  32768  \n",
       "14           OpenAI    True             448        None                    448  \n",
       "8            Google    True            8192        None                   8192  \n",
       "15             Meta    True          131072        None                 131072  \n",
       "3            OpenAI    True             448        None                    448  \n",
       "19             Meta    True            8192        None                   8192  \n",
       "2      Hugging Face    True             448        None                    448  \n",
       "0              Meta    True            8192        None                   8192  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "pd.DataFrame(response.json()['data']).sort_values(['created'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b438489-c102-434b-872d-e807169deef6",
   "metadata": {
    "id": "2b438489-c102-434b-872d-e807169deef6"
   },
   "source": [
    "The Groq client object enables interaction with the Groq REST API and a chat completion request is made via the client.chat.completions.create method.\n",
    "\n",
    "The most important arguments of the client.chat.completions.create method are the following:\n",
    "* messages: a list of messages (dictionary form) that make up the conversation to date\n",
    "* model: a string indicating which model to use (see [list of models](https://console.groq.com/docs/models))\n",
    "* max_completion_tokens: the maximum number of tokens that are generated in the chat completion\n",
    "* response_format: setting this to `{ \"type\": \"json_object\" }` enables JSON output\n",
    "* seed: sample deterministically as best as possible, though identical outputs each time are not guaranteed\n",
    "* temperature: between 0 and 2 where higher values like 0.8 make the output more random (creative) and values like 0.2 are more focused and deterministic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "99216392-acc3-4a00-826b-c166a1e52534",
   "metadata": {
    "id": "99216392-acc3-4a00-826b-c166a1e52534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create in module groq.resources.chat.completions:\n",
      "\n",
      "create(*, messages: 'Iterable[ChatCompletionMessageParam]', model: \"Union[str, Literal['gemma2-9b-it', 'llama-3.3-70b-versatile', 'llama-3.1-8b-instant', 'llama-guard-3-8b', 'llama3-70b-8192', 'llama3-8b-8192']]\", exclude_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, frequency_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, function_call: 'Optional[completion_create_params.FunctionCall] | NotGiven' = NOT_GIVEN, functions: 'Optional[Iterable[completion_create_params.Function]] | NotGiven' = NOT_GIVEN, include_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, logit_bias: 'Optional[Dict[str, int]] | NotGiven' = NOT_GIVEN, logprobs: 'Optional[bool] | NotGiven' = NOT_GIVEN, max_completion_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, max_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, metadata: 'Optional[Dict[str, str]] | NotGiven' = NOT_GIVEN, n: 'Optional[int] | NotGiven' = NOT_GIVEN, parallel_tool_calls: 'Optional[bool] | NotGiven' = NOT_GIVEN, presence_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, reasoning_effort: \"Optional[Literal['none', 'default']] | NotGiven\" = NOT_GIVEN, reasoning_format: \"Optional[Literal['hidden', 'raw', 'parsed']] | NotGiven\" = NOT_GIVEN, response_format: 'Optional[completion_create_params.ResponseFormat] | NotGiven' = NOT_GIVEN, search_settings: 'Optional[completion_create_params.SearchSettings] | NotGiven' = NOT_GIVEN, seed: 'Optional[int] | NotGiven' = NOT_GIVEN, service_tier: \"Optional[Literal['auto', 'on_demand', 'flex']] | NotGiven\" = NOT_GIVEN, stop: 'Union[Optional[str], List[str], None] | NotGiven' = NOT_GIVEN, store: 'Optional[bool] | NotGiven' = NOT_GIVEN, stream: 'Optional[Literal[False]] | Literal[True] | NotGiven' = NOT_GIVEN, temperature: 'Optional[float] | NotGiven' = NOT_GIVEN, tool_choice: 'Optional[ChatCompletionToolChoiceOptionParam] | NotGiven' = NOT_GIVEN, tools: 'Optional[Iterable[ChatCompletionToolParam]] | NotGiven' = NOT_GIVEN, top_logprobs: 'Optional[int] | NotGiven' = NOT_GIVEN, top_p: 'Optional[float] | NotGiven' = NOT_GIVEN, user: 'Optional[str] | NotGiven' = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'ChatCompletion | Stream[ChatCompletionChunk]' method of groq.resources.chat.completions.Completions instance\n",
      "    Creates a model response for the given chat conversation.\n",
      "\n",
      "    Args:\n",
      "      messages: A list of messages comprising the conversation so far.\n",
      "\n",
      "      model: ID of the model to use. For details on which models are compatible with the Chat\n",
      "          API, see available [models](https://console.groq.com/docs/models)\n",
      "\n",
      "      exclude_domains: Deprecated: Use search_settings.exclude_domains instead. A list of domains to\n",
      "          exclude from the search results when the model uses a web search tool.\n",
      "\n",
      "      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n",
      "          existing frequency in the text so far, decreasing the model's likelihood to\n",
      "          repeat the same line verbatim.\n",
      "\n",
      "      function_call: Deprecated in favor of `tool_choice`.\n",
      "\n",
      "          Controls which (if any) function is called by the model. `none` means the model\n",
      "          will not call a function and instead generates a message. `auto` means the model\n",
      "          can pick between generating a message or calling a function. Specifying a\n",
      "          particular function via `{\"name\": \"my_function\"}` forces the model to call that\n",
      "          function.\n",
      "\n",
      "          `none` is the default when no functions are present. `auto` is the default if\n",
      "          functions are present.\n",
      "\n",
      "      functions: Deprecated in favor of `tools`.\n",
      "\n",
      "          A list of functions the model may generate JSON inputs for.\n",
      "\n",
      "      include_domains: Deprecated: Use search_settings.include_domains instead. A list of domains to\n",
      "          include in the search results when the model uses a web search tool.\n",
      "\n",
      "      logit_bias: This is not yet supported by any of our models. Modify the likelihood of\n",
      "          specified tokens appearing in the completion.\n",
      "\n",
      "      logprobs: This is not yet supported by any of our models. Whether to return log\n",
      "          probabilities of the output tokens or not. If true, returns the log\n",
      "          probabilities of each output token returned in the `content` of `message`.\n",
      "\n",
      "      max_completion_tokens: The maximum number of tokens that can be generated in the chat completion. The\n",
      "          total length of input tokens and generated tokens is limited by the model's\n",
      "          context length.\n",
      "\n",
      "      max_tokens: Deprecated in favor of `max_completion_tokens`. The maximum number of tokens\n",
      "          that can be generated in the chat completion. The total length of input tokens\n",
      "          and generated tokens is limited by the model's context length.\n",
      "\n",
      "      metadata: This parameter is not currently supported.\n",
      "\n",
      "      n: How many chat completion choices to generate for each input message. Note that\n",
      "          the current moment, only n=1 is supported. Other values will result in a 400\n",
      "          response.\n",
      "\n",
      "      parallel_tool_calls: Whether to enable parallel function calling during tool use.\n",
      "\n",
      "      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on\n",
      "          whether they appear in the text so far, increasing the model's likelihood to\n",
      "          talk about new topics.\n",
      "\n",
      "      reasoning_effort: this field is only available for qwen3 models. Set to 'none' to disable\n",
      "          reasoning. Set to 'default' or null to let Qwen reason.\n",
      "\n",
      "      reasoning_format: Specifies how to output reasoning tokens\n",
      "\n",
      "      response_format: An object specifying the format that the model must output. Setting to\n",
      "          `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs\n",
      "          which ensures the model will match your supplied JSON schema. json_schema\n",
      "          response format is only supported on llama 4 models. Setting to\n",
      "          `{ \"type\": \"json_object\" }` enables the older JSON mode, which ensures the\n",
      "          message the model generates is valid JSON. Using `json_schema` is preferred for\n",
      "          models that support it.\n",
      "\n",
      "      search_settings: Settings for web search functionality when the model uses a web search tool.\n",
      "\n",
      "      seed: If specified, our system will make a best effort to sample deterministically,\n",
      "          such that repeated requests with the same `seed` and parameters should return\n",
      "          the same result. Determinism is not guaranteed, and you should refer to the\n",
      "          `system_fingerprint` response parameter to monitor changes in the backend.\n",
      "\n",
      "      service_tier: The service tier to use for the request. Defaults to `on_demand`.\n",
      "\n",
      "          - `auto` will automatically select the highest tier available within the rate\n",
      "            limits of your organization.\n",
      "          - `flex` uses the flex tier, which will succeed or fail quickly.\n",
      "\n",
      "      stop: Up to 4 sequences where the API will stop generating further tokens. The\n",
      "          returned text will not contain the stop sequence.\n",
      "\n",
      "      store: This parameter is not currently supported.\n",
      "\n",
      "      stream: If set, partial message deltas will be sent. Tokens will be sent as data-only\n",
      "          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n",
      "          as they become available, with the stream terminated by a `data: [DONE]`\n",
      "          message. [Example code](/docs/text-chat#streaming-a-chat-completion).\n",
      "\n",
      "      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n",
      "          make the output more random, while lower values like 0.2 will make it more\n",
      "          focused and deterministic. We generally recommend altering this or top_p but not\n",
      "          both.\n",
      "\n",
      "      tool_choice: Controls which (if any) tool is called by the model. `none` means the model will\n",
      "          not call any tool and instead generates a message. `auto` means the model can\n",
      "          pick between generating a message or calling one or more tools. `required` means\n",
      "          the model must call one or more tools. Specifying a particular tool via\n",
      "          `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n",
      "          call that tool.\n",
      "\n",
      "          `none` is the default when no tools are present. `auto` is the default if tools\n",
      "          are present.\n",
      "\n",
      "      tools: A list of tools the model may call. Currently, only functions are supported as a\n",
      "          tool. Use this to provide a list of functions the model may generate JSON inputs\n",
      "          for. A max of 128 functions are supported.\n",
      "\n",
      "      top_logprobs: This is not yet supported by any of our models. An integer between 0 and 20\n",
      "          specifying the number of most likely tokens to return at each token position,\n",
      "          each with an associated log probability. `logprobs` must be set to `true` if\n",
      "          this parameter is used.\n",
      "\n",
      "      top_p: An alternative to sampling with temperature, called nucleus sampling, where the\n",
      "          model considers the results of the tokens with top_p probability mass. So 0.1\n",
      "          means only the tokens comprising the top 10% probability mass are considered. We\n",
      "          generally recommend altering this or temperature but not both.\n",
      "\n",
      "      user: A unique identifier representing your end-user, which can help us monitor and\n",
      "          detect abuse.\n",
      "\n",
      "      extra_headers: Send extra headers\n",
      "\n",
      "      extra_query: Add additional query parameters to the request\n",
      "\n",
      "      extra_body: Add additional JSON properties to the request\n",
      "\n",
      "      timeout: Override the client-level default timeout for this request, in seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7",
   "metadata": {
    "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7"
   },
   "source": [
    "As a first example, note how the messages input is given as a list of a dictionaries with `role` and `content` keys. This is in a ChatML format recognised by many LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81",
   "metadata": {
    "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models are artificial intelligence (AI) systems that process and generate human-like language. They work by:\n",
      "\n",
      "1. **Training**: Being trained on massive amounts of text data, which helps them learn patterns and relationships in language.\n",
      "2. **Tokenization**: Breaking down text into small units called tokens, such as words or characters.\n",
      "3. **Contextualization**: Analyzing the context in which tokens are used to understand their meaning.\n",
      "4. **Prediction**: Using complex algorithms to predict the next token or generate text based on the context.\n",
      "\n",
      "This process allows large language models to learn, understand, and generate human-like language, enabling applications such as text generation, language translation, and conversational AI.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {   \"role\": \"system\", # sets the persona of the model\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": \"Explain briefly how large language models work\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b60c2-4300-4338-b325-c3ea876c1afe",
   "metadata": {
    "id": "728b60c2-4300-4338-b325-c3ea876c1afe"
   },
   "source": [
    "The output is in Markdown format so the following line formats this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "88845bb3-4a3e-403a-93ab-439c46aa1832",
   "metadata": {
    "id": "88845bb3-4a3e-403a-93ab-439c46aa1832"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large language models are artificial intelligence (AI) systems that process and generate human-like language. They work by:\n",
       "\n",
       "1. **Training**: Being trained on massive amounts of text data, which helps them learn patterns and relationships in language.\n",
       "2. **Tokenization**: Breaking down text into small units called tokens, such as words or characters.\n",
       "3. **Contextualization**: Analyzing the context in which tokens are used to understand their meaning.\n",
       "4. **Prediction**: Using complex algorithms to predict the next token or generate text based on the context.\n",
       "\n",
       "This process allows large language models to learn, understand, and generate human-like language, enabling applications such as text generation, language translation, and conversational AI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8c97b-1711-4355-b108-86bed769c109",
   "metadata": {
    "id": "e1c8c97b-1711-4355-b108-86bed769c109"
   },
   "source": [
    "## Text summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4",
   "metadata": {
    "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4"
   },
   "source": [
    "We start with a llama3-8b-8192, a model using just over 8 billion parameters with at most 8192 tokens produced as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2",
   "metadata": {
    "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2"
   },
   "source": [
    "Here is an article to be summarised from the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29",
   "metadata": {
    "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29"
   },
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco area Friday at 4:42 a.m. PT (7:42 a.m. ET), the U.S. Geological Survey reported. The quake left about 2,000 customers without power, said David Eisenhower, a spokesman for Pacific Gas and Light. Under the USGS classification, a magnitude 4.2 earthquake is considered \"light,\" which it says usually causes minimal damage. \"We had quite a spike in calls, mostly calls of inquiry, none of any injury, none of any damage that was reported,\" said Capt. Al Casciato of the San Francisco police. \"It was fairly mild.\" Watch police describe concerned calls immediately after the quake Â» . The quake was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles, the USGS said. Oakland is just east of San Francisco, across San Francisco Bay. An Oakland police dispatcher told CNN the quake set off alarms at people's homes. The shaking lasted about 50 seconds, said CNN meteorologist Chad Myers. According to the USGS, magnitude 4.2 quakes are felt indoors and may break dishes and windows and overturn unstable objects. Pendulum clocks may stop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754257a-74ff-4747-a18e-7ebfbc82639c",
   "metadata": {
    "id": "9754257a-74ff-4747-a18e-7ebfbc82639c"
   },
   "source": [
    "**Exercise:**\n",
    "Summarise the story text using the following three prompts. Use the format given above but here there is no need to set the persona (i.e. only include one dictionary in the messages list when calling `client.chat.completions.create`.) Comment on any differences.\n",
    "\n",
    "1) \"Summarise the following article in 3 sentences.\"\n",
    "\n",
    "2) \"Give me a TL;DR of this text.\"\n",
    "\n",
    "3) \"What's the key takeaway here?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9c372155-bdde-45a4-a184-fce4c4e8034c",
   "metadata": {
    "id": "9c372155-bdde-45a4-a184-fce4c4e8034c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      " Summarise the following article in 3 sentences.  \n",
      " Here is a summary of the article in 3 sentences:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area on Friday morning, with its epicenter located about two miles east-northeast of Oakland. The quake caused minimal damage and only briefly disrupted power for about 2,000 customers, with no reported injuries or significant damage. The shaking lasted around 50 seconds and was described as \"fairly mild\" by authorities, with most calls to emergency services being inquiries rather than reports of damage or injury.\n",
      "-------------------------------------------------- \n",
      " Give me a TL;DR of this text.  \n",
      " Here is a brief summary of the text:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area at 4:42 a.m. local time, causing minimal damage and power outages for about 2,000 customers. The quake was considered \"light\" and lasted around 50 seconds, with reports of no injuries or significant damage.\n",
      "-------------------------------------------------- \n",
      " What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake shook the San Francisco area, causing some power outages and reports of noise and alarm activation, but no significant damage or injuries were reported, and it was considered a \"light\" earthquake with minimal impact.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Summarise the following article in 3 sentences. \", \"Give me a TL;DR of this text. \", \"What's the key takeaway here?\"]\n",
    "#content will be p + story for p in prompts\n",
    "\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": p + story}],\n",
    "                model=\"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "    print('-'*50, '\\n', p, '\\n', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df",
   "metadata": {
    "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df"
   },
   "source": [
    "Run the above code again below and note that the answers may differ. This is due to the probabilistic nature of LLM token generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14",
   "metadata": {
    "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      " Summarise the following article in 3 sentences.  \n",
      " Here is a summary of the article in 3 sentences:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area at 4:42 a.m. on Friday, shaking for about 50 seconds and leaving around 2,000 customers without power. The US Geological Survey classified the earthquake as \"light\", meaning it usually causes minimal damage, and initial reports showed no injuries or damage reported to authorities. The quake was centered near Oakland, east of San Francisco, and while it may have caused some disturbance, it was described as \"fairly mild\" by local officials.\n",
      "-------------------------------------------------- \n",
      " Give me a TL;DR of this text.  \n",
      " A magnitude 4.2 earthquake struck the San Francisco area at 4:42am, causing about 2,000 customers to lose power, but no injuries or major damage were reported. The quake, considered \"light\" by the USGS, was centered near Oakland and lasted about 50 seconds, with some residents reporting alarms going off at their homes.\n",
      "-------------------------------------------------- \n",
      " What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake struck the San Francisco area early Friday morning, causing minimal damage and no reported injuries, but affecting about 2,000 customers' power and triggering alarm systems at people's homes.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Summarise the following article in 3 sentences. \", \"Give me a TL;DR of this text. \", \"What's the key takeaway here?\"]\n",
    "#content will be p + story for p in prompts\n",
    "\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": p + story}],\n",
    "                model=\"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "    print('-'*50, '\\n', p, '\\n', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f",
   "metadata": {
    "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f"
   },
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf",
   "metadata": {
    "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf"
   },
   "source": [
    "**Exercise**: In this section adjust the `max_completion_tokens` and `temperature` settings below to obtain different responses. Show some examples with the prompt \"Continue the story: It was a great time to be alive\" with the model \"llama-3.1-8b-instant\".\n",
    "\n",
    "* max_completion_tokens - the maximum number of tokens to generate. Note that longer words are made of multiple tokens (set to 200 and 500)\n",
    "* temperature (positive number) - the higher the number the more random (creative) the output (set to 0.2, 0.8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "50207704-94be-4309-8a82-dc1d22a063ee",
   "metadata": {
    "id": "50207704-94be-4309-8a82-dc1d22a063ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive. The sun was shining, the birds were singing, and the air was filled with the sweet scent of blooming flowers. People were laughing and smiling, eager to start their day. It was a time of hope and opportunity, a time when anything seemed possible.\n",
      "\n",
      "As you walked down the street, you couldn't help but feel the energy of the city. The streets were bustling with activity, people rushing to and fro, heading to work or school or on a day out. The sound of construction echoed through the air, as new buildings and roads were being built to keep up with the growing population.\n",
      "\n",
      "It was a time of great technological advancements. Cars, trains, and planes zoomed through the air, carrying people and goods with ease. And with the help of computers and the internet, people were connected like never before.\n",
      "\n",
      "But it wasn't just the technology that made this time great. It was also the sense of community that thrived. People came\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=200, do not have a temperature setting)\n",
    "prompt = \"Continue the story: It was a great time to be alive\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_completion_tokens = 200,\n",
    "    model = \"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "63235fcc-3a61-4829-998b-37f2f104322b",
   "metadata": {
    "id": "63235fcc-3a61-4829-998b-37f2f104322b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive. The year was 2050, and the world was vastly different from the one we knew just a few decades prior. Cities were now sprawling metropolises with towering skyscrapers that seemed to touch the sky, and the air was crisp and clean, thanks to the widespread adoption of advanced air purification systems.\n",
      "\n",
      "Humanity had also made tremendous strides in medicine, with diseases like cancer and Alzheimer's relegated to the history books. People were living longer, healthier lives, and the quality of life had improved dramatically.\n",
      "\n",
      "But it wasn't all sunshine and rainbows. There were still challenges to be faced, of course. Climate change remained a pressing issue, and there were still pockets of poverty and inequality scattered throughout the globe.\n",
      "\n",
      "Despite these remaining problems, the general mood was one of optimism and excitement. The world was changing at an incredible pace, and people were eager to see what the future held.\n",
      "\n",
      "As I walked through the city, I couldn't help but feel a sense of awe at the sheer scale and complexity of the urban landscape. Towering above me were skyscrapers that seemed to defy gravity, their exteriors covered in intricate latticework of pipes and wires.\n",
      "\n",
      "I passed by a group of people gathered around a large screen, watching as a team of robots worked together to assemble a new skyscraper. The robots moved with precision and speed, their mechanical arms dancing across the concrete as they built the structure piece by piece.\n",
      "\n",
      "As I continued on my way, I came across a group of people gathered around a small park. They were all sitting on benches, watching as a group of children played with a team of small, remotely controlled robots. The robots were designed to look like animals, and they were scurrying around the park, interacting with the children and playing games with them.\n",
      "\n",
      "It was a truly surreal scene, one that I never could have imagined just a few decades earlier. And yet, here we were, living in a world that was full of wonders and surprises.\n",
      "\n",
      "As I walked, I couldn't help but think about the incredible advancements that humanity had made. From the Mars colonies to the advanced artificial intelligence systems, it was clear that we were capable of achieving incredible things.\n",
      "\n",
      "And yet, despite all of these amazing advancements, there was still a sense of uncertainty in the air. There were still so many questions about what the future held, and what challenges lay ahead.\n",
      "\n",
      "But as I looked around at the bustling streets,\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=500, do not have a temperature setting)\n",
    "response = client.chat.completions.create(\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_completion_tokens = 500,\n",
    "    model = \"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50",
   "metadata": {
    "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive. The sun was shining, the birds were singing, and the world seemed to be full of endless possibilities. I was 25 years old, with a bright future ahead of me. I had just landed my dream job at a prestigious tech company, and I was excited to start my new role.\n",
      "\n",
      "As I walked to work, I couldn't help but feel a sense of pride and accomplishment. I had worked hard to get to this point, and it felt amazing to know that all my efforts had paid off. The city was bustling with energy, and I felt like I was a part of it all.\n",
      "\n",
      "When I arrived at the office, my colleagues welcomed me with open arms. They were a talented and diverse group of people, and I was excited to learn from them and contribute to the team. Our company was known for its innovative approach to technology, and I was eager to be a part of it.\n",
      "\n",
      "As the day went on, I found myself getting more and more excited about my new role. I was given a lot of responsibility, and I was determined to make the most of it. I spent hours pouring over code, brainstorming ideas, and collaborating with my team.\n",
      "\n",
      "At lunchtime, I met up with some friends from college. We caught up on each other's lives, shared stories, and laughed together. It was great to see them, and I felt grateful for the strong bonds we had formed.\n",
      "\n",
      "After lunch, I headed back to the office, feeling refreshed and rejuvenated. The afternoon was filled with meetings, presentations, and problem-solving. I was in my element, and I loved every minute of it.\n",
      "\n",
      "As the day drew to a close, I felt a sense of satisfaction and fulfillment. I had accomplished a lot, and I knew that I was exactly where I was meant to be. I left the office feeling proud of myself, and I couldn't wait to see what the future held.\n",
      "\n",
      "As I walked home, I couldn't help but think about how great it was to be alive. The world was full of challenges, but it was also full of opportunities. I felt grateful for the life I had, and I was excited to see what the future held.\n",
      "\n",
      "As I drifted off to sleep that night, I smiled to myself, feeling grateful for this moment in time. It was a great time to be alive, and I was excited to see what the future held.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 0.2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature = 0.2, #more focused and deterministic\n",
    "    model = \"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d",
   "metadata": {
    "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As I gazed out at the bustling city, I couldn't help but feel a sense of excitement and possibility. The sun was shining, the air was crisp, and the smell of fresh-cut grass wafted through the streets. It was a great time to be alive, and I felt grateful to be experiencing it.\n",
      "\n",
      "I was sitting in a small cafÃ©, sipping on a latte and watching the world go by. The sounds of chatter and laughter filled the air, and I couldn't help but join in. I felt a sense of community and connection with the people around me, and it made me feel grateful to be a part of this world.\n",
      "\n",
      "As I sat there, I couldn't help but think about all the amazing things that were happening in the world right now. New scientific discoveries were being made, new technologies were being developed, and new forms of art and music were emerging. It was a time of great change and progress, and I felt lucky to be a part of it.\n",
      "\n",
      "But as I sat there, lost in my own thoughts, I couldn't help but feel a sense of unease. There were still so many problems in the world, so many challenges that needed to be overcome. I thought about the wars, the poverty, the inequality, and the injustice that still plagued our world. And I knew that we still had a long way to go before we could truly say that we were living in a utopia.\n",
      "\n",
      "Despite the challenges, I still felt a sense of hope. I knew that as long as we had people like me, people who were willing to work towards making the world a better place, things would get better. I knew that as long as we had people like me, who were dedicated to fighting for what was right, we would overcome those challenges and build a better future.\n",
      "\n",
      "And so, I sipped on my latte and smiled to myself, feeling grateful to be alive and to be a part of such a great time.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 1, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature = 1, #semi-random\n",
    "    model = \"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf",
   "metadata": {
    "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf"
   },
   "source": [
    "Note what happens when the temperature is set too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "931f8e00-928c-4218-8e74-8c56269fbfcb",
   "metadata": {
    "id": "931f8e00-928c-4218-8e74-8c56269fbfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive. Joey leaned back in his creaky office chair, swivel-twisting around in its worn canvas coating as he perched unsteadily between enthusiasm and trepidation. He wasn't certain, some twenty years future-looking-out from this moment in high school commencement just taken, what spectaculaes tomorrow would rustling forth, how some others close by he now huzz-o' it all -- though this '67 America in varevitch.\n",
      "\n",
      "Some would remark `what nosh shucks! FUTURE`. New 'gaining air and wobble, into shivers here ! It a, this â so life-ruffling events pander well wharfly, once only us re-stold this epochâ¦â You need feel too a new: better to â just but most and toâ¦to where: you no-one new again a time.\n",
      "Even more were the 2 who gazed each after's, just and.\n",
      "Had Joey never or given when time in but -- just his better choice.\n",
      "\n",
      "Citing few were, and, after â then no âem both: said hazy.\n",
      "but â So âunquiet ' it b in what I from of as, n .\n",
      "Tears I like from another their bough -'! Like anyâs â Just p like out , beâs\n",
      "to. Told they after from have was - also is say \"When'd. But \" That which have say my from tell You ! \"\n",
      "\n",
      "But I hope my turn next would bring .\n",
      "Time went now -- no there â when of its be too; \n",
      "after whens, few is same âJust let,\" âmâ¦ my go. and were â at new!\n",
      "but h some. me so â out let for find here \"with me .\n",
      "I want show one loveâ on th here with. Those time for. Then let can us do more say.\n",
      "\n",
      "âthough on no give of .\n",
      "if it t.\n",
      "out to the s here if `the new ` to do â when you new call new e `When wereâ¦ what our forâ in after all ; which let 'from be this ever give; The from call we like, then no give will; I â in The The with -- first --- If not out say \"â¦ Butâs say That. But first there it have... the some here if one my get take would also you too\".\n",
      "Those as good to help âof\", some when about like look to e make what kind have happen to than from now too, but The; some help; other look like but s parting time \"To my pâ th âso so ` âdo no. e when i âit e;â The you & my heart on on of time` or never, â is just us find âlove... t day also there was\"â In If my was i ! that \"And ' ... i -- This one like .â If never too much .\n",
      "Just could happen i be other what next all ! and . the in by look, you I butâ¦\". That these were also once no these b n.\n",
      "\n",
      "It takes someone quite `there you âl want other' make in it you wish more take you take? \"And is of these our be have -- after be too that 'if they'll so new w no I you! love\".\n",
      "Here that one else like I the after these people one there who know to g his best friend till; always first can take too by show` his my new own that . All âno my and; one; -- some with who âno ! first now there let find be.â at day other people like by you show` who look doâ `What new it âm the` if âSo? next. for of not --- --- a i'll n is it here part for `my they have then --- wh So ch give could from do my âor s give which have never said we s no â¦e see Iâ `No r` one; *at**n after then while so next all --- ``And`` n\" So` -- this - p my h. there never `with their in allâ out In each my -- of` first same As say âyou m `â the âmy. That can I f My in My -â So -- be new.\n",
      "That You did When âis - Time was And No see p In let Me when My f You t a it of you me have.\n",
      "There said part th of .\n",
      "At my tell .\n",
      "See till, see I... of Then from , would if here call day. we You me have and.\n",
      "\n",
      "Then `The` come look' 10 some of do any as best sheâs; part a -- otherâs to always up not; day so ; then never stay night before want be --- some people âNew! say I on â you ! and in my here\".\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature = 2, #randomest\n",
    "    model = \"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c",
   "metadata": {
    "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c"
   },
   "source": [
    "### Zero-shot and one-short prompting for question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6662f0a-c7e3-4235-972d-2771781a4e53",
   "metadata": {
    "id": "f6662f0a-c7e3-4235-972d-2771781a4e53"
   },
   "source": [
    "This section shows the impact of prompting on the response. Zero-shot prompting means we provide the prompt without any examples or additional context. Let us initially ask Mistral a question using no prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc",
   "metadata": {
    "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The way two chemicals react depends on the specific chemical properties and the conditions in which they interact. Here are the general steps involved in a chemical reaction:\n",
       "\n",
       "1. **Collision**: Two molecules (atoms or ions) must collide with each other. The likelihood of a collision depends on the concentration and movement of the molecules.\n",
       "2. **Orientation**: The molecules must be oriented in a way that allows them to interact. This is influenced by the shape and polarity of the molecules.\n",
       "3. **Energy**: The molecules must have enough energy to overcome the repulsive forces between them and form a new bond.\n",
       "4. **Reaction**: If the molecules meet the above conditions, a chemical reaction occurs, resulting in the formation of new products.\n",
       "\n",
       "There are several types of chemical reactions, including:\n",
       "\n",
       "1. **Synthesis**: Two or more molecules combine to form a new molecule.\n",
       "Example: 2H2 + O2 â 2H2O\n",
       "2. **Decomposition**: A single molecule breaks down into simpler molecules.\n",
       "Example: 2H2O â 2H2 + O2\n",
       "3. **Single displacement**: One element displaces another element from a compound.\n",
       "Example: Zn + CuSO4 â ZnSO4 + Cu\n",
       "4. **Double displacement**: Two compounds exchange partners.\n",
       "Example: NaCl + AgNO3 â NaNO3 + AgCl\n",
       "5. **Combustion**: A molecule reacts with oxygen to release energy.\n",
       "Example: CH4 + 2O2 â CO2 + 2H2O\n",
       "\n",
       "To predict how two chemicals will react, you can use the following factors:\n",
       "\n",
       "1. **Electronegativity**: The tendency of an atom to attract electrons.\n",
       "2. **Polarity**: The distribution of electrons within a molecule.\n",
       "3. **Reactivity series**: A list of elements arranged by their reactivity.\n",
       "4. **Acid-base properties**: The ability of a molecule to donate or accept a proton.\n",
       "\n",
       "It's essential to note that predicting chemical reactions involves a complex interplay of these factors and requires a strong understanding of chemistry principles."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d",
   "metadata": {
    "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d"
   },
   "source": [
    "**Exercise:** Ask the same question but modify the prompt to return the answer to the same question in a simpler form (still using the llama-3.1-8b-instant model). Experiment with different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6",
   "metadata": {
    "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chemical reactions occur when two chemicals interact, resulting in a change in their structure. This change is usually driven by the transfer of electrons, bonds breaking, or forming new ones. Energy is often released or absorbed in the process, leading to a new compound or products with different properties."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain in 50 words or less: How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04676a11-57a7-4f9c-87e2-128559f0ffce",
   "metadata": {
    "id": "04676a11-57a7-4f9c-87e2-128559f0ffce"
   },
   "source": [
    "### One-shot prompting ###\n",
    "\n",
    "Next, note the dramatic change when we give the following template setting a new role and providing an English question followed by a French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473",
   "metadata": {
    "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment deux substances chimiques rÃ©agissent-elles?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"system\",\n",
    "             \"content\": \"You translate English to French.\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"What time is it?\"}, #this time example provides a sample so the model can follow the pattern\n",
    "               {\"role\": \"assistant\",\n",
    "               \"content\": \"Quelle heure est-il?\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269",
   "metadata": {
    "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269"
   },
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050",
   "metadata": {
    "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050"
   },
   "source": [
    "Recall that since the text generation process outputs one token at a time, their outputs often need adjusting. This is where examples can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b",
   "metadata": {
    "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regrettably, I will be unable to attend the meeting. I apologize for any inconvenience this may cause.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"I'm gonna head out now, see you later.\"\n",
    "response1 = \"I will be leaving now. See you later.\"\n",
    "\n",
    "prompt2 =  \"That movie was super cool!\"\n",
    "response2 = \"The movie was very impressive.\"\n",
    "\n",
    "prompt3 = \"Can't make it to the meeting, sorry.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional editor. Rewrite casual sentences into a formal tone.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e9e18-cc27-453b-8844-3f683fb607f8",
   "metadata": {
    "id": "040e9e18-cc27-453b-8844-3f683fb607f8"
   },
   "source": [
    "The output can also be moulded to provide SQL output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ed854534-30db-4745-b910-27d12a3ed47e",
   "metadata": {
    "id": "ed854534-30db-4745-b910-27d12a3ed47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM products WHERE quantity = 0;\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"Show me all users who signed up in the last 30 days.\"\n",
    "response1 = \"SELECT * FROM users WHERE signup_date >= CURRENT_DATE - INTERVAL '30 days';\"\n",
    "\n",
    "prompt2 = \"What is the average order value?\"\n",
    "response2 =  \"SELECT AVG(order_total) FROM orders;\"\n",
    "\n",
    "prompt3 = \"List products that are out of stock.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that translates natural language to SQL.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7",
   "metadata": {
    "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7"
   },
   "source": [
    "**Exercise**: Create a few examples to train the \"llama3-70b-8192\" LLM to take in user content in the form below and provide output as a pandas dataframe. Use the `exec` function to execute its output to display the answer of sample input as a data frame.\n",
    "\n",
    "Example:\n",
    "\n",
    "given the user content\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "| col1 | col2 | col3\n",
    "\n",
    "| 32 | 27 | 25\n",
    "\n",
    "| 64 | 23 | 14\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train the model to output\n",
    "\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2b8ac08a-5759-41cd-a560-e77694573723",
   "metadata": {
    "id": "2b8ac08a-5759-41cd-a560-e77694573723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"\"\"\n",
    "\n",
    "| col1 | col2 | col3\n",
    "\n",
    "| 40 | 7 | 27\n",
    "\n",
    "| 10 | 48 | 15\n",
    "\n",
    "\"\"\"\n",
    "response1 = \"df = pd.DataFrame({'col1': [40, 10], 'col2': [7, 28], 'col3': [27, 15]})\"\n",
    "\n",
    "prompt2 = \"\"\"\n",
    "\n",
    "| col1 | col2 | col3\n",
    "\n",
    "| 32 | 27 | 25\n",
    "\n",
    "| 64 | 23 | 14\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a data scientist, provide single code line to convert input into a pandas dataframe dictionary\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f62a88-c7b4-4431-acda-97e9a98981f4",
   "metadata": {
    "id": "29f62a88-c7b4-4431-acda-97e9a98981f4"
   },
   "source": [
    "Also show what happens when the question is asked in the absence of a system role and without few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6",
   "metadata": {
    "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you're trying to display a table with three columns (`col1`, `col2`, and `col3`) and two rows of data.\n",
      "\n",
      "Here's a formatted version of your table:\n",
      "\n",
      "| col1 | col2 | col3 |\n",
      "| --- | --- | --- |\n",
      "| 32  | 27  | 25  |\n",
      "| 64  | 23  | 14  |\n",
      "\n",
      "Let me know if there's anything else I can help you with!\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660884-dfd4-4b96-a65d-d21ddbf99423",
   "metadata": {
    "id": "28660884-dfd4-4b96-a65d-d21ddbf99423"
   },
   "source": [
    "### Chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f85ef-afc0-4e31-9ad8-ae780c535837",
   "metadata": {
    "id": "724f85ef-afc0-4e31-9ad8-ae780c535837"
   },
   "source": [
    "The results of question-answering can also be improved by prompting the LLM to provide intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee04ad6-968b-406a-8119-981b01ef5f92",
   "metadata": {
    "id": "bee04ad6-968b-406a-8119-981b01ef5f92"
   },
   "source": [
    "**Exercise**: Using the following prompts, compare the answers of the \"llama3-8b-8192\" model (set seed=21). (If this model is no longer available choose a model with relatively few parameters.)\n",
    "\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb",
   "metadata": {
    "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      " How many s's are in the word 'success'? \n",
      " There are 2 s's in the word \"success\".\n",
      "-------------------------------------------------- \n",
      " How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn. \n",
      " Let's go through each letter of the word \"success\" to count the number of s's:\n",
      "\n",
      "1. S (first letter)\n",
      "2. U\n",
      "3. C\n",
      "4. C\n",
      "5. E\n",
      "6. S\n",
      "7. S\n",
      "\n",
      "As we go through the letters, we find two S's (at positions 1 and 6) and three other letters. Therefore, the correct answer is:\n",
      "\n",
      "There are 2 S's in the word \"success\".\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"How many s's are in the word 'success'?\", \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\"]\n",
    "\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                messages = [{\"role\": \"user\", \"content\": p}],\n",
    "                model = \"llama3-8b-8192\",\n",
    "                seed = 21\n",
    ")\n",
    "\n",
    "    print('-'*50, '\\n', p, '\\n', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f",
   "metadata": {
    "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f"
   },
   "source": [
    "## Comparison of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f",
   "metadata": {
    "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f"
   },
   "source": [
    "**Exercise**: Compare the performance of 2 LLMs by outputting the answers of the following questions into a dataframe.\n",
    "\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "\n",
    "Column headings:\n",
    "\n",
    "Model Name | Question | Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238",
   "metadata": {
    "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist break up with the statistician? \\n\\nBecause they had too many arguments about p-values! ð  \\n\\n\\nLet me know if you'd like to hear another one! ð</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>Here's a way to calculate 22 * 13 mentally using a breakdown method:\\n\\n**1. Break down 22:** Think of 22 as (20 + 2)\\n\\n**2. Distribute:** Now you have  (20 + 2) * 13.  Distribute the 13:\\n   * 20 * 13 = 260 \\n   * 2 * 13 = 26\\n\\n**3. Add:** Add the two products: 260 + 26 = 286\\n\\n\\n**Therefore, 22 * 13 = 286**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>Bartholomew Buttercup, a cherub with an unruly tuft of blonde hair and a bottomless appetite for mashed sweet potatoes, was on a mission.  His mission? To conquer the floor.\\n\\nHe couldn't quite understand why his legs refused to move as one. The world was a fascinating place, just out of reach â fluffy rugs begging to be crawled upon, a sparkly mobile dangling enticingly, and the elusive comfort of Mom's lap. \\n\\nOne sunny afternoon, while his chubby fingers tugged at a dangling teething toy, an idea, bright and unyielding as the sun shining through the nursery window, struck Bartholomew. He pushed himself up, his eyebrows furrowing in concentration, and let out a triumphant gurgle. This, he decided, was the key.\\n\\nFor the next few hours, Bartholomew \"skated\" â a wobbly, jerky motion fueled by desperation and mashed sweet potato-induced energy. He propelled himself across the floor, arms flailing like windmill wings, legs kicking blindly. \\n\\nEach failed attempt resulted in a frustrating topple, met with tears that were swiftly soothed by a cooing melody from Mom, followed by gentle pats and encouraging \"good tries.\" \\n\\nFrustration was brewing. He wanted to be like his older sister, Lily, who glided across the floor like a gazelle on ice skates. But Bartholomew wasn't a gazelle. He was a Buttercup, and Buttercups, he decided, were meant to conquer the world, one wobbly crawl at a time.\\n\\nHe took a deep breath, remembering the way his sister moved. He watched her, his big brown eyes full of determination. \\n\\nSlowly, methodically, he began to coordinate his movements. One hand reached out, pushing against the rug. The other, mirroring the action, found purchase on the wooden floor. His legs, finally understanding their role, pushed down with surprising strength. \\n\\nHe felt a surge of triumph as he moved â not a jerky skate, but a smooth, controlled crawl. He was moving! He was conquering!\\n\\nA roar of glee erupted from his tiny throat. His legs pumped, propelling him across the room. He was a speedboat on a sunny sea, navigating the world with newfound freedom.\\n\\nHe stopped short, realizing his destination. Mom sat on the floor, a book open in her lap, humming a soft tune. \\n\\nWith a final burst of energy, Bartholomew reached her, collapsing into her open arms. \\n\\n\"Yay, Bartholomew!\" Mom exclaimed, her voice filled with pride. \"You crawled!\"\\n\\nBartholomew gurgled happily, his face flushed with exertion and triumph. He had conquered the floor. But more importantly, he had conquered the doubt, proving to himself that even the smallest Buttercup could achieve anything they set their mind to.  The world, now within reach, beckoned.  And Bartholomew Buttercup, he was ready.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist's cat join a band?\\n\\nBecause it wanted to be the purr-cussionist and help with the data correlation, but it ended up having a regression problem with its rhythm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>To mentally calculate 22 * 13, you can use the method of breaking down the multiplication into simpler operations. \\n\\nHere's one way to do it:\\n\\n1. Break down 13 into (10 + 3)\\n2. Multiply 22 by 10: 22 * 10 = 220\\n3. Multiply 22 by 3: 22 * 3 = 66\\n4. Add the two results together: 220 + 66 = 286\\n\\nSo, 22 * 13 is mentally calculable as 286.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>Once upon a time, in a cozy little house on a sunny street, there lived a baby named Luna. Luna was a bundle of energy, always moving, always exploring, and always getting into mischief. But there was one thing that Luna hadn't mastered yet: crawling.\\n\\nEvery day, Luna would watch her older siblings as they played, laughed, and crawled on all fours. She'd try to join in, but her little arms and legs would flail and flop, and she'd end up on her bottom, giggling.\\n\\nLuna's mom, Emma, would kneel down beside her and say, \"Don't worry, little one. You'll get it eventually.\" But Luna was determined to learn. She practiced and practiced, using the couch cushions as her launching pad to try and reach the toys on the floor.\\n\\nOne day, Emma had an idea. She grabbed a few colorful blankets and spread them out in front of Luna, creating a mini obstacle course. \"Try crawling over these, Luna!\" she exclaimed. Luna's eyes widened as she gazed at the blankets. This was the challenge she'd been waiting for.\\n\\nWith a determined look on her face, Luna started to move. At first, it was more like a flop than a crawl. Her arms and legs flailed wildly, and she fell onto her bottom with a tiny oof. But Emma was patient, cheering her on, \"Keep going, Luna! You're doing it!\"\\n\\nUndeterred, Luna tried again. This time, she managed to push herself forward with one hand and drag her other leg behind her. Then, suddenly, she found herself on all fours, her legs moving in a wobbly rhythm.\\n\\nLuna's eyes sparkled with excitement as she realized she was actually crawling! Emma and her siblings cheered and clapped, \"Woo-hoo, Luna! Look at you go!\" Luna giggled and wobbled a little more, her tiny hands and feet finding their balance.\\n\\nAs the days went by, Luna became more confident, her crawling becoming smoother and more efficient. She'd scamper across the room, laughing and chattering to herself, her tail wagging like crazy (even though she didn't have a tail!).\\n\\nOne day, Luna's mom had had enough of Luna's endless crawling practice. \"Okay, kiddo,\" she said with a mischievous grin, \"time for an adventure!\" Emma pulled out a tiny stroller and gently helped Luna inside. Luna squealed with delight as Emma pushed her down the hallway, watching as her baby girl explored the world from a new perspective.\\n\\nLuna's first crawl was a milestone moment in her life. She realized that even the toughest challenges could be overcome with practice, patience, and a little bit of momma-love. And as she settled into her stroller, her mom by her side, Luna knew that this was just the beginning of a lifetime of adventures, discovery, and crawling into the heart of the fun.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model Name  \\\n",
       "0          gemma2-9b-it   \n",
       "1          gemma2-9b-it   \n",
       "2          gemma2-9b-it   \n",
       "3  llama-3.1-8b-instant   \n",
       "4  llama-3.1-8b-instant   \n",
       "5  llama-3.1-8b-instant   \n",
       "\n",
       "                                                 Question  \\\n",
       "0                      Tell me a joke about data science.   \n",
       "1                 How can one calculate 22 * 13 mentally?   \n",
       "2  Write a creative story about a baby learning to crawl.   \n",
       "3                      Tell me a joke about data science.   \n",
       "4                 How can one calculate 22 * 13 mentally?   \n",
       "5  Write a creative story about a baby learning to crawl.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Why did the data scientist break up with the statistician? \\n\\nBecause they had too many arguments about p-values! ð  \\n\\n\\nLet me know if you'd like to hear another one! ð  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Here's a way to calculate 22 * 13 mentally using a breakdown method:\\n\\n**1. Break down 22:** Think of 22 as (20 + 2)\\n\\n**2. Distribute:** Now you have  (20 + 2) * 13.  Distribute the 13:\\n   * 20 * 13 = 260 \\n   * 2 * 13 = 26\\n\\n**3. Add:** Add the two products: 260 + 26 = 286\\n\\n\\n**Therefore, 22 * 13 = 286**  \n",
       "2  Bartholomew Buttercup, a cherub with an unruly tuft of blonde hair and a bottomless appetite for mashed sweet potatoes, was on a mission.  His mission? To conquer the floor.\\n\\nHe couldn't quite understand why his legs refused to move as one. The world was a fascinating place, just out of reach â fluffy rugs begging to be crawled upon, a sparkly mobile dangling enticingly, and the elusive comfort of Mom's lap. \\n\\nOne sunny afternoon, while his chubby fingers tugged at a dangling teething toy, an idea, bright and unyielding as the sun shining through the nursery window, struck Bartholomew. He pushed himself up, his eyebrows furrowing in concentration, and let out a triumphant gurgle. This, he decided, was the key.\\n\\nFor the next few hours, Bartholomew \"skated\" â a wobbly, jerky motion fueled by desperation and mashed sweet potato-induced energy. He propelled himself across the floor, arms flailing like windmill wings, legs kicking blindly. \\n\\nEach failed attempt resulted in a frustrating topple, met with tears that were swiftly soothed by a cooing melody from Mom, followed by gentle pats and encouraging \"good tries.\" \\n\\nFrustration was brewing. He wanted to be like his older sister, Lily, who glided across the floor like a gazelle on ice skates. But Bartholomew wasn't a gazelle. He was a Buttercup, and Buttercups, he decided, were meant to conquer the world, one wobbly crawl at a time.\\n\\nHe took a deep breath, remembering the way his sister moved. He watched her, his big brown eyes full of determination. \\n\\nSlowly, methodically, he began to coordinate his movements. One hand reached out, pushing against the rug. The other, mirroring the action, found purchase on the wooden floor. His legs, finally understanding their role, pushed down with surprising strength. \\n\\nHe felt a surge of triumph as he moved â not a jerky skate, but a smooth, controlled crawl. He was moving! He was conquering!\\n\\nA roar of glee erupted from his tiny throat. His legs pumped, propelling him across the room. He was a speedboat on a sunny sea, navigating the world with newfound freedom.\\n\\nHe stopped short, realizing his destination. Mom sat on the floor, a book open in her lap, humming a soft tune. \\n\\nWith a final burst of energy, Bartholomew reached her, collapsing into her open arms. \\n\\n\"Yay, Bartholomew!\" Mom exclaimed, her voice filled with pride. \"You crawled!\"\\n\\nBartholomew gurgled happily, his face flushed with exertion and triumph. He had conquered the floor. But more importantly, he had conquered the doubt, proving to himself that even the smallest Buttercup could achieve anything they set their mind to.  The world, now within reach, beckoned.  And Bartholomew Buttercup, he was ready.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Why did the data scientist's cat join a band?\\n\\nBecause it wanted to be the purr-cussionist and help with the data correlation, but it ended up having a regression problem with its rhythm.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               To mentally calculate 22 * 13, you can use the method of breaking down the multiplication into simpler operations. \\n\\nHere's one way to do it:\\n\\n1. Break down 13 into (10 + 3)\\n2. Multiply 22 by 10: 22 * 10 = 220\\n3. Multiply 22 by 3: 22 * 3 = 66\\n4. Add the two results together: 220 + 66 = 286\\n\\nSo, 22 * 13 is mentally calculable as 286.  \n",
       "5                                                 Once upon a time, in a cozy little house on a sunny street, there lived a baby named Luna. Luna was a bundle of energy, always moving, always exploring, and always getting into mischief. But there was one thing that Luna hadn't mastered yet: crawling.\\n\\nEvery day, Luna would watch her older siblings as they played, laughed, and crawled on all fours. She'd try to join in, but her little arms and legs would flail and flop, and she'd end up on her bottom, giggling.\\n\\nLuna's mom, Emma, would kneel down beside her and say, \"Don't worry, little one. You'll get it eventually.\" But Luna was determined to learn. She practiced and practiced, using the couch cushions as her launching pad to try and reach the toys on the floor.\\n\\nOne day, Emma had an idea. She grabbed a few colorful blankets and spread them out in front of Luna, creating a mini obstacle course. \"Try crawling over these, Luna!\" she exclaimed. Luna's eyes widened as she gazed at the blankets. This was the challenge she'd been waiting for.\\n\\nWith a determined look on her face, Luna started to move. At first, it was more like a flop than a crawl. Her arms and legs flailed wildly, and she fell onto her bottom with a tiny oof. But Emma was patient, cheering her on, \"Keep going, Luna! You're doing it!\"\\n\\nUndeterred, Luna tried again. This time, she managed to push herself forward with one hand and drag her other leg behind her. Then, suddenly, she found herself on all fours, her legs moving in a wobbly rhythm.\\n\\nLuna's eyes sparkled with excitement as she realized she was actually crawling! Emma and her siblings cheered and clapped, \"Woo-hoo, Luna! Look at you go!\" Luna giggled and wobbled a little more, her tiny hands and feet finding their balance.\\n\\nAs the days went by, Luna became more confident, her crawling becoming smoother and more efficient. She'd scamper across the room, laughing and chattering to herself, her tail wagging like crazy (even though she didn't have a tail!).\\n\\nOne day, Luna's mom had had enough of Luna's endless crawling practice. \"Okay, kiddo,\" she said with a mischievous grin, \"time for an adventure!\" Emma pulled out a tiny stroller and gently helped Luna inside. Luna squealed with delight as Emma pushed her down the hallway, watching as her baby girl explored the world from a new perspective.\\n\\nLuna's first crawl was a milestone moment in her life. She realized that even the toughest challenges could be overcome with practice, patience, and a little bit of momma-love. And as she settled into her stroller, her mom by her side, Luna knew that this was just the beginning of a lifetime of adventures, discovery, and crawling into the heart of the fun.  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None) # allows wide dataframes to be viewed\n",
    "models = [\"gemma2-9b-it\", \"llama-3.1-8b-instant\"] #can edit this\n",
    "\n",
    "prompts = [\"Tell me a joke about data science.\",\n",
    "\"How can one calculate 22 * 13 mentally?\",\n",
    "\"Write a creative story about a baby learning to crawl.\"\n",
    "]\n",
    "\n",
    "results = {'Model Name': [], 'Question': [], 'Answer': []}\n",
    "\n",
    "for model in models:\n",
    "    for prompt in prompts:\n",
    "        results['Model Name'].append(model)\n",
    "        results['Question'].append(prompt)\n",
    "        output = client.chat.completions.create(model = model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        results['Answer'].append(output.choices[0].message.content.strip())\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5",
   "metadata": {
    "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5"
   },
   "source": [
    "### Bonus\n",
    "\n",
    "See if you can prompt an LLM to perform sentiment analysis (output 'Positive' or 'Negative' only) on a given piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a",
   "metadata": {
    "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": 'Identify the sentiment of \"That movie was interesting.\" Output positive or negative only.'},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d",
   "metadata": {
    "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279421d-cbca-4003-941e-c2d2bc2833a8",
   "metadata": {
    "id": "f279421d-cbca-4003-941e-c2d2bc2833a8"
   },
   "source": [
    "We worked with a few Large Language Models (LLMs) using Groq and experimented with prompting for summarisation, text completion and question-answering tasks.\n",
    "\n",
    "We also explored controlling the randomness (creativity) of output through the temperature setting and tried different types of prompting to achieve desired forms of output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f06d5e-7073-485b-b046-afe839ab844c",
   "metadata": {
    "id": "94f06d5e-7073-485b-b046-afe839ab844c"
   },
   "source": [
    "## References\n",
    "1. [Groq's prompting guide](https://console.groq.com/docs/prompting)\n",
    "2. [Groq's playground](https://console.groq.com/playground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61aab4a-1330-4762-9318-5635c3a97aa7",
   "metadata": {
    "id": "d61aab4a-1330-4762-9318-5635c3a97aa7"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2025 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
